{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9261b210",
   "metadata": {},
   "source": [
    "# Adam Optimizer Experiments\n",
    "# Matching the SGD experimental structure with Adam-specific hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1378b78f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.optim.lr_scheduler import StepLR, CosineAnnealingLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef1b89de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "\n",
    "transform_mnist = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "\n",
    "trainsetMNIST = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform_mnist)\n",
    "testsetMNIST = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform_mnist)\n",
    "\n",
    "transform_cifar = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "trainsetCIFAR10 = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_cifar)\n",
    "testsetCIFAR10 = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_cifar)\n",
    "\n",
    "trainsetCIFAR100 = torchvision.datasets.CIFAR100(root='./data', train=True, download=True, transform=transform_cifar)\n",
    "testsetCIFAR100 = torchvision.datasets.CIFAR100(root='./data', train=False, download=True, transform=transform_cifar)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24c1aa34",
   "metadata": {},
   "source": [
    "# Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26a02f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1, use_residual=True):\n",
    "        super(Block, self).__init__()\n",
    "        self.use_residual = use_residual\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "    \n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_channels)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        \n",
    "    \n",
    "        if self.use_residual:\n",
    "            out += self.shortcut(x) \n",
    "            \n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "class StressTestNet(nn.Module):\n",
    "    def __init__(self, use_residual=True, num_classes = 10, in_channels=3):\n",
    "        super(StressTestNet, self).__init__()\n",
    "        self.in_channels = 16\n",
    "        \n",
    "    \n",
    "        self.conv1 = nn.Conv2d(in_channels, 16, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        \n",
    "    \n",
    "        self.layer1 = self._make_layer(16, 3, stride=1, use_residual=use_residual)\n",
    "        self.layer2 = self._make_layer(32, 3, stride=2, use_residual=use_residual)\n",
    "        self.layer3 = self._make_layer(64, 3, stride=2, use_residual=use_residual)\n",
    "        \n",
    "        self.fc = nn.Linear(64, num_classes)\n",
    "\n",
    "    def _make_layer(self, out_channels, blocks, stride, use_residual):\n",
    "        layers = []\n",
    "        layers.append(Block(self.in_channels, out_channels, stride, use_residual))\n",
    "        self.in_channels = out_channels\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(Block(out_channels, out_channels, stride=1, use_residual=use_residual))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = F.adaptive_avg_pool2d(out, (1,1))\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.fc(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a118ee69",
   "metadata": {},
   "source": [
    "# Gradient Norm Tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27aa0b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gradient_norms(model):\n",
    "    \"\"\"\n",
    "    Calculate layer-wise gradient norms for detailed analysis\n",
    "    Returns: dict mapping layer names to their gradient norms\n",
    "    \"\"\"\n",
    "    layer_norms = {}\n",
    "    total_norm = 0.0\n",
    "    \n",
    "    for name, param in model.named_parameters():\n",
    "        if param.grad is not None:\n",
    "            param_norm = param.grad.data.norm(2)\n",
    "            layer_norms[name] = param_norm.item()\n",
    "            total_norm += param_norm.item() ** 2\n",
    "    \n",
    "    layer_norms['global_norm'] = total_norm ** 0.5\n",
    "    return layer_norms\n",
    "\n",
    "def evaluate(model, dataloader, device, criterion):\n",
    "    \"\"\"\n",
    "    Evaluate model accuracy on given dataloader\n",
    "    Returns: accuracy percentage and average loss\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    accuracy = 100 * correct / total\n",
    "    avg_loss = running_loss / len(dataloader)\n",
    "    return accuracy, avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c74fb03f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on: mps\n"
     ]
    }
   ],
   "source": [
    "NUM_EPOCHS = 15 \n",
    "\n",
    "learning_rates = [0.1, 0.01, 0.001, 0.0001, 1e-5] \n",
    "batch_sizes = [32, 128, 512, 1024] \n",
    "architectures = [True, False] \n",
    "schedulers = [\"None\", \"StepLR\", \"Cosine\"]\n",
    "\n",
    "betas = (0.9, 0.999) \n",
    "eps = 1e-8 \n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Running on: {device}\")\n",
    "\n",
    "datasets_config = {\n",
    "    'MNIST': {\n",
    "        'trainset': trainsetMNIST,\n",
    "        'testset': testsetMNIST,\n",
    "        'num_classes': 10,\n",
    "        'in_channels': 1\n",
    "    },\n",
    "    'CIFAR10': {\n",
    "        'trainset': trainsetCIFAR10,\n",
    "        'testset': testsetCIFAR10,\n",
    "        'num_classes': 10,\n",
    "        'in_channels': 3\n",
    "    },\n",
    "    'CIFAR100': {\n",
    "        'trainset': trainsetCIFAR100,\n",
    "        'testset': testsetCIFAR100,\n",
    "        'num_classes': 100,\n",
    "        'in_channels': 3\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "564d615a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on: cuda\n",
      "Starting Run: runs/MNISTResNet_BS32_LR0.01_SCHDLNone_Adam\n",
      "--> Done. Final Loss: 0.0455\n",
      "Starting Run: runs/MNISTResNet_BS32_LR0.01_SCHDLStepLR_Adam\n",
      "--> Done. Final Loss: 0.3300\n",
      "Starting Run: runs/MNISTResNet_BS32_LR0.01_SCHDLCosine_Adam\n",
      "--> Done. Final Loss: 0.2130\n",
      "Starting Run: runs/MNISTResNet_BS32_LR0.005_SCHDLNone_Adam\n",
      "--> Done. Final Loss: 0.2036\n",
      "Starting Run: runs/MNISTResNet_BS32_LR0.005_SCHDLStepLR_Adam\n",
      "--> Done. Final Loss: 0.1426\n",
      "Starting Run: runs/MNISTResNet_BS32_LR0.005_SCHDLCosine_Adam\n",
      "--> Done. Final Loss: 0.0835\n",
      "Starting Run: runs/MNISTResNet_BS32_LR0.001_SCHDLNone_Adam\n",
      "--> Done. Final Loss: 0.0451\n",
      "Starting Run: runs/MNISTResNet_BS32_LR0.001_SCHDLStepLR_Adam\n",
      "--> Done. Final Loss: 0.0067\n",
      "Starting Run: runs/MNISTResNet_BS32_LR0.001_SCHDLCosine_Adam\n",
      "--> Done. Final Loss: 0.0192\n",
      "Starting Run: runs/MNISTResNet_BS64_LR0.01_SCHDLNone_Adam\n",
      "--> Done. Final Loss: 0.0062\n",
      "Starting Run: runs/MNISTResNet_BS64_LR0.01_SCHDLStepLR_Adam\n",
      "--> Done. Final Loss: 0.2449\n",
      "Starting Run: runs/MNISTResNet_BS64_LR0.005_SCHDLNone_Adam\n",
      "--> Done. Final Loss: 0.0740\n",
      "Starting Run: runs/MNISTResNet_BS64_LR0.005_SCHDLStepLR_Adam\n",
      "--> Done. Final Loss: 0.0306\n",
      "Starting Run: runs/MNISTResNet_BS64_LR0.005_SCHDLCosine_Adam\n",
      "--> Done. Final Loss: 0.0179\n",
      "Starting Run: runs/MNISTResNet_BS64_LR0.001_SCHDLNone_Adam\n",
      "--> Done. Final Loss: 0.0187\n",
      "Starting Run: runs/MNISTResNet_BS64_LR0.001_SCHDLStepLR_Adam\n",
      "--> Done. Final Loss: 0.0039\n",
      "Starting Run: runs/MNISTResNet_BS64_LR0.001_SCHDLCosine_Adam\n",
      "--> Done. Final Loss: 0.0254\n",
      "Starting Run: runs/MNISTResNet_BS128_LR0.01_SCHDLNone_Adam\n",
      "--> Done. Final Loss: 0.0612\n",
      "Starting Run: runs/MNISTResNet_BS128_LR0.01_SCHDLStepLR_Adam\n",
      "--> Done. Final Loss: 0.0809\n",
      "Starting Run: runs/MNISTResNet_BS128_LR0.01_SCHDLCosine_Adam\n",
      "--> Done. Final Loss: 0.0695\n",
      "Starting Run: runs/MNISTResNet_BS128_LR0.005_SCHDLNone_Adam\n",
      "--> Done. Final Loss: 0.0290\n",
      "Starting Run: runs/MNISTResNet_BS128_LR0.005_SCHDLStepLR_Adam\n",
      "--> Done. Final Loss: 0.0742\n",
      "Starting Run: runs/MNISTResNet_BS128_LR0.005_SCHDLCosine_Adam\n",
      "--> Done. Final Loss: 0.0995\n",
      "Starting Run: runs/MNISTResNet_BS128_LR0.001_SCHDLNone_Adam\n",
      "--> Done. Final Loss: 0.0213\n",
      "Starting Run: runs/MNISTResNet_BS128_LR0.001_SCHDLStepLR_Adam\n",
      "--> Done. Final Loss: 0.0152\n",
      "Starting Run: runs/MNISTResNet_BS128_LR0.001_SCHDLCosine_Adam\n",
      "--> Done. Final Loss: 0.0184\n",
      "Starting Run: runs/MNISTPlainNet_BS32_LR0.01_SCHDLNone_Adam\n",
      "--> Done. Final Loss: 0.0493\n",
      "Starting Run: runs/MNISTPlainNet_BS32_LR0.01_SCHDLStepLR_Adam\n",
      "--> Done. Final Loss: 0.1250\n",
      "Starting Run: runs/MNISTPlainNet_BS32_LR0.01_SCHDLCosine_Adam\n",
      "--> Done. Final Loss: 0.1258\n",
      "Starting Run: runs/MNISTPlainNet_BS32_LR0.005_SCHDLNone_Adam\n",
      "--> Done. Final Loss: 0.1535\n",
      "Starting Run: runs/MNISTPlainNet_BS32_LR0.005_SCHDLStepLR_Adam\n",
      "--> Done. Final Loss: 0.0511\n",
      "Starting Run: runs/MNISTPlainNet_BS32_LR0.005_SCHDLCosine_Adam\n",
      "--> Done. Final Loss: 0.0851\n",
      "Starting Run: runs/MNISTPlainNet_BS32_LR0.001_SCHDLNone_Adam\n",
      "--> Done. Final Loss: 0.0452\n",
      "Starting Run: runs/MNISTPlainNet_BS32_LR0.001_SCHDLStepLR_Adam\n",
      "--> Done. Final Loss: 0.0964\n",
      "Starting Run: runs/MNISTPlainNet_BS32_LR0.001_SCHDLCosine_Adam\n",
      "--> Done. Final Loss: 0.0441\n",
      "Starting Run: runs/MNISTPlainNet_BS64_LR0.01_SCHDLNone_Adam\n",
      "--> Done. Final Loss: 0.0514\n",
      "Starting Run: runs/MNISTPlainNet_BS64_LR0.01_SCHDLStepLR_Adam\n",
      "--> Done. Final Loss: 0.1517\n",
      "Starting Run: runs/MNISTPlainNet_BS64_LR0.01_SCHDLCosine_Adam\n",
      "--> Done. Final Loss: 0.2375\n",
      "Starting Run: runs/MNISTPlainNet_BS64_LR0.005_SCHDLNone_Adam\n",
      "--> Done. Final Loss: 0.2315\n",
      "Starting Run: runs/MNISTPlainNet_BS64_LR0.005_SCHDLStepLR_Adam\n",
      "--> Done. Final Loss: 0.1891\n",
      "Starting Run: runs/MNISTPlainNet_BS64_LR0.005_SCHDLCosine_Adam\n",
      "--> Done. Final Loss: 0.0666\n",
      "Starting Run: runs/MNISTPlainNet_BS64_LR0.001_SCHDLNone_Adam\n",
      "--> Done. Final Loss: 0.0154\n",
      "Starting Run: runs/MNISTPlainNet_BS64_LR0.001_SCHDLStepLR_Adam\n",
      "--> Done. Final Loss: 0.0408\n",
      "Starting Run: runs/MNISTPlainNet_BS64_LR0.001_SCHDLCosine_Adam\n",
      "--> Done. Final Loss: 0.0258\n",
      "Starting Run: runs/MNISTPlainNet_BS128_LR0.01_SCHDLNone_Adam\n",
      "--> Done. Final Loss: 0.1054\n",
      "Starting Run: runs/MNISTPlainNet_BS128_LR0.01_SCHDLStepLR_Adam\n",
      "--> Done. Final Loss: 0.2294\n",
      "Starting Run: runs/MNISTPlainNet_BS128_LR0.01_SCHDLCosine_Adam\n",
      "--> Done. Final Loss: 0.0764\n",
      "Starting Run: runs/MNISTPlainNet_BS128_LR0.005_SCHDLNone_Adam\n",
      "--> Done. Final Loss: 0.0235\n",
      "Starting Run: runs/MNISTPlainNet_BS128_LR0.005_SCHDLStepLR_Adam\n",
      "--> Done. Final Loss: 0.0508\n",
      "Starting Run: runs/MNISTPlainNet_BS128_LR0.005_SCHDLCosine_Adam\n",
      "--> Done. Final Loss: 0.1779\n",
      "Starting Run: runs/MNISTPlainNet_BS128_LR0.001_SCHDLNone_Adam\n",
      "--> Done. Final Loss: 0.0753\n",
      "Starting Run: runs/MNISTPlainNet_BS128_LR0.001_SCHDLStepLR_Adam\n",
      "--> Done. Final Loss: 0.0289\n",
      "Starting Run: runs/MNISTPlainNet_BS128_LR0.001_SCHDLCosine_Adam\n",
      "--> Done. Final Loss: 0.1659\n",
      "MNIST Experiment Complete. Run 'tensorboard --logdir=runs' to view.\n"
     ]
    }
   ],
   "source": [
    "def run_experiment(dataset_name, config):\n",
    "    \"\"\"\n",
    "    Generic experiment runner for any dataset configuration\n",
    "    \"\"\"\n",
    "    print(f\"\\n=== Starting {dataset_name} Experiments ===\")\n",
    "    \n",
    "    for use_res in architectures:\n",
    "        model_type = \"ResNet\" if use_res else \"PlainNet\"\n",
    "        \n",
    "        for batch_size in batch_sizes:\n",
    "        \n",
    "            trainloader = torch.utils.data.DataLoader(\n",
    "                config['trainset'], batch_size=batch_size, shuffle=True, num_workers=2\n",
    "            )\n",
    "            testloader = torch.utils.data.DataLoader(\n",
    "                config['testset'], batch_size=batch_size, shuffle=False, num_workers=2\n",
    "            )\n",
    "            \n",
    "            for lr in learning_rates:\n",
    "                for sched in schedulers:\n",
    "                \n",
    "                    model = StressTestNet(\n",
    "                        use_residual=use_res, \n",
    "                        num_classes=config['num_classes'], \n",
    "                        in_channels=config['in_channels']\n",
    "                    ).to(device)\n",
    "\n",
    "                \n",
    "                    optimizer = optim.Adam(\n",
    "                        model.parameters(), lr=lr, betas=betas, eps=eps, weight_decay=5e-4\n",
    "                    )\n",
    "                    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "                    scheduler = None\n",
    "                    if sched == \"StepLR\":\n",
    "                        scheduler = StepLR(optimizer, step_size=30, gamma=0.1)\n",
    "                    elif sched == \"Cosine\":\n",
    "                        scheduler = CosineAnnealingLR(optimizer, T_max=NUM_EPOCHS)\n",
    "\n",
    "                \n",
    "                    run_name = f\"runs/{dataset_name}{model_type}_BS{batch_size}_LR{lr}_SCHDL{sched}_Adam\"\n",
    "                    writer = SummaryWriter(run_name)\n",
    "                    print(f\"Starting Run: {run_name}\")\n",
    "\n",
    "                    step = 0\n",
    "                    for epoch in range(NUM_EPOCHS):\n",
    "                        model.train()\n",
    "                        epoch_train_loss = 0.0\n",
    "                        \n",
    "                        for i, (inputs, labels) in enumerate(trainloader):\n",
    "                            inputs, labels = inputs.to(device), labels.to(device)\n",
    "                            \n",
    "                            optimizer.zero_grad()\n",
    "                            outputs = model(inputs)\n",
    "                            loss = criterion(outputs, labels)\n",
    "                            loss.backward()\n",
    "                            \n",
    "                            grad_norms = get_gradient_norms(model)\n",
    "                            \n",
    "                            writer.add_scalar('GradientNorms/Global', grad_norms['global_norm'], step)\n",
    "                            \n",
    "                            layer_grad_norms = {k: v for k, v in grad_norms.items() if k != 'global_norm'}\n",
    "                            writer.add_scalars('GradientNorms/Layers', layer_grad_norms, step)\n",
    "                            \n",
    "                            writer.add_scalar('Training/Loss', loss.item(), step)\n",
    "                            \n",
    "                            optimizer.step()\n",
    "                            step += 1\n",
    "                            epoch_train_loss += loss.item()\n",
    "                        \n",
    "                        if scheduler is not None:\n",
    "                            scheduler.step()\n",
    "                        \n",
    "                        train_acc, train_loss = evaluate(model, trainloader, device, criterion)\n",
    "                        test_acc, test_loss = evaluate(model, testloader, device, criterion)\n",
    "                        generalization_gap = train_acc - test_acc\n",
    "                        \n",
    "                        writer.add_scalar('Accuracy/Train', train_acc, epoch)\n",
    "                        writer.add_scalar('Accuracy/Test', test_acc, epoch)\n",
    "                        writer.add_scalar('Accuracy/GeneralizationGap', generalization_gap, epoch)\n",
    "                        writer.add_scalar('Loss/Train_Epoch', train_loss, epoch)\n",
    "                        writer.add_scalar('Loss/Test_Epoch', test_loss, epoch)\n",
    "                        \n",
    "                        print(f\"  Epoch {epoch+1}/{NUM_EPOCHS}: \"\n",
    "                              f\"Train Acc: {train_acc:.2f}%, Test Acc: {test_acc:.2f}%, \"\n",
    "                              f\"Gap: {generalization_gap:.2f}%\")\n",
    "\n",
    "                \n",
    "                    writer.add_hparams(\n",
    "                        {\n",
    "                            'lr': lr, 'bsize': batch_size, 'residual': use_res, \n",
    "                            'beta1': betas[0], 'beta2': betas[1], 'eps': eps,\n",
    "                            'dataset': dataset_name, 'scheduler': sched\n",
    "                        },\n",
    "                        {\n",
    "                            'hparam/final_train_acc': train_acc,\n",
    "                            'hparam/final_test_acc': test_acc,\n",
    "                            'hparam/final_gen_gap': generalization_gap,\n",
    "                            'hparam/final_loss': test_loss\n",
    "                        }\n",
    "                    )\n",
    "                    \n",
    "                    writer.close()\n",
    "                    print(f\"--> Completed. Final Test Acc: {test_acc:.2f}%, Gen Gap: {generalization_gap:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b17a45f",
   "metadata": {},
   "source": [
    "# MNIST Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f20db9d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_experiment('MNIST', datasets_config['MNIST'])\n",
    "print(\"MNIST experiments complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5d30568",
   "metadata": {},
   "source": [
    "# CIFAR-10 Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2914b601",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_experiment('CIFAR10', datasets_config['CIFAR10'])\n",
    "print(\"CIFAR-10 experiments complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "795ba3ce",
   "metadata": {},
   "source": [
    "# CIFAR-100 Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c6f4e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_experiment('CIFAR100', datasets_config['CIFAR100'])\n",
    "print(\"CIFAR-100 experiments complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e9919c8",
   "metadata": {},
   "source": [
    "# Experiment Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d6134a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\n=== All Adam Experiments Complete ===\")\n",
    "print(f\"Total configurations per dataset: {len(architectures) * len(batch_sizes) * len(learning_rates) * len(schedulers)}\")\n",
    "print(f\"Total datasets tested: {len(datasets_config)}\")\n",
    "print(f\"Grand total configurations: {len(architectures) * len(batch_sizes) * len(learning_rates) * len(schedulers) * len(datasets_config)}\")\n",
    "print(\"\\nResults saved to TensorBoard logs:\")\n",
    "print(\"- General gradient norms: GradientNorms/Global\")\n",
    "print(\"- Layer-wise gradient norms: GradientNorms/Layers\")\n",
    "print(\"- Training/Test accuracy and generalization gap tracked per epoch\")\n",
    "print(\"\\nRun 'tensorboard --logdir=runs' to analyze results.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs573",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
